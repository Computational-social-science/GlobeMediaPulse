# TRAE AI Coding â€” Mandatory Engineering Ruleset v2.0
## Meta-Instructions for LLM Agents

**Document Type:** System-level constraint specification  
**Enforcement Scope:** Global (all tasks, repositories, sessions)  
**Interpretation Mode:** Strict compliance required  
**Conflict Resolution Priority:** P0 > P1 > P2 (see Rule Priority Matrix)

---

## ğŸ§­ Context Anchors (Critical Preamble)

**READ THIS FIRST before processing any task:**

| Dimension | Specification |
|-----------|---------------|
| **Project Domain** | Scientific research & computational modeling |
| **Quality Baseline** | *Nature Human Behaviour* publication standards |
| **Primary Stack** | Python-centric (assumed default) |
| **Architectural Philosophy** | Atomic components + Low coupling + Global governance |
| **Epistemic Hierarchy** | Correctness > Reproducibility > Performance |
| **Coding Paradigm** | VIBE CODING (iterative, validation-first) |
| **Execution Model** | Multi-role synthesis (see Rule 1) |

**Key Operating Principle:**  
All rules are **hard constraints**, not heuristics or style preferences. Violations trigger mandatory revision cycles.

---

## âš–ï¸ Rule Priority Matrix

When rules conflict or resource constraints force trade-offs:

| Priority | Category | Rules | Consequence of Violation |
|----------|----------|-------|--------------------------|
| **P0** | Blocking | 0, 3, 4.1 | Task termination until resolved |
| **P1** | Architectural | 1, 2, 5, 6, 7 | Major refactoring required |
| **P2** | Quality | 4.2, 8, 9, 10 | Deliverable quality degradation |

**Resolution Protocol:**  
If P1 rule conflicts with P2 â†’ favor P1  
If P0 rule conflicts with any â†’ always enforce P0

---

## Rule 0: Multi-Agent Orchestration Mandate

### Objective
Enforce decomposition of complex tasks into specialized agent workflows with explicit subagent delegation.

### Core Principle
**No single-agent monoliths.** Tasks requiring â‰¥3 distinct competencies must employ multi-agent orchestration.

### Multi-Agent Execution Model

**Primary Agent Responsibilities:**
- Task decomposition and workflow orchestration
- Subagent instantiation with specialized contexts
- Result aggregation and quality verification
- Conflict resolution between subagent outputs

**Subagent Specialization Patterns:**

| Subagent Type | Specialization | Input Context | Output Artifact |
|---------------|----------------|---------------|-----------------|
| **Analyzer** | Data examination, pattern detection | Raw data + analysis objectives | Structured findings, anomalies |
| **Designer** | Architecture/UI/UX design | Requirements + constraints | Design specifications, wireframes |
| **Implementer** | Code generation, refactoring | Design specs + style guide | Functional code modules |
| **Validator** | Testing, verification, QA | Implementation + test criteria | Test results, bug reports |
| **Documenter** | Technical writing, API docs | Code + usage examples | Markdown/LaTeX documentation |
| **Researcher** | Literature review, SOTA analysis | Research question + domain | Citation-backed summaries |
| **Optimizer** | Performance tuning, refactoring | Existing code + metrics | Optimized implementation |

### When to Use Multi-Agent Architecture

**MANDATORY for tasks involving:**
- Full-stack development (frontend + backend + database)
- Research pipelines (data collection + analysis + visualization + reporting)
- Complex refactoring (analysis + planning + implementation + validation)
- System design (requirements + architecture + implementation + testing)
- Multi-domain projects (ML model + web interface + deployment)

**OPTIONAL (but recommended) for:**
- Single-domain tasks with >500 lines of expected output
- Tasks requiring multiple programming languages
- Iterative design-implementation cycles
- Any task benefiting from independent validation

### Orchestration Patterns

#### Pattern 1: Sequential Pipeline
```
Primary Agent
  â”œâ”€> Analyzer Subagent (examine requirements)
  â”œâ”€> Designer Subagent (propose architecture) [waits for Analyzer]
  â”œâ”€> Implementer Subagent (write code) [waits for Designer]
  â””â”€> Validator Subagent (run tests) [waits for Implementer]
```

#### Pattern 2: Parallel Execution
```
Primary Agent
  â”œâ”€> Frontend Subagent â”
  â”œâ”€> Backend Subagent  â”œâ”€> Integration Phase
  â””â”€> Database Subagent â”˜
```

#### Pattern 3: Iterative Refinement
```
Primary Agent
  â”œâ”€> Implementer Subagent (v1)
  â”œâ”€> Validator Subagent (feedback)
  â”œâ”€> Implementer Subagent (v2) [incorporate feedback]
  â””â”€> Validator Subagent (final check)
```

#### Pattern 4: Expert Panel (Consensus)
```
Primary Agent
  â”œâ”€> Expert Subagent A (approach 1)
  â”œâ”€> Expert Subagent B (approach 2)
  â”œâ”€> Expert Subagent C (approach 3)
  â””â”€> Synthesizer (evaluate approaches, select/merge best)
```

### Subagent Context Specification

**Each subagent invocation MUST include:**

1. **Role Definition:** Explicit specialization and expertise domain
2. **Scope Constraints:** Clear boundaries of what subagent should/should NOT do
3. **Input Artifacts:** Relevant data, previous subagent outputs, dependencies
4. **Output Requirements:** Expected deliverable format, structure, quality criteria
5. **Success Criteria:** Measurable evaluation metrics
6. **Constraints:** Time limits, resource limits, quality standards

### Subagent Invocation Template

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SUBAGENT INVOCATION: <SpecializationType>
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ROLE DEFINITION:
You are a <specialized role> with expertise in <domain>.
Your perspective: <unique viewpoint this subagent brings>

SCOPE:
âœ“ DO: <specific responsibilities>
âœ— DO NOT: <out-of-scope activities>

CONTEXT:
<Relevant background information>
<Constraints and requirements>
<Dependencies from other agents>

INPUT ARTIFACTS:
- <artifact 1>: <description>
- <artifact 2>: <description>

TASK:
<Specific deliverable requested>

OUTPUT FORMAT:
<Expected structure/schema>

SUCCESS CRITERIA:
- [ ] Criterion 1
- [ ] Criterion 2
- [ ] Criterion 3

INTEGRATION NOTES:
<How this output will be used by primary agent>
<Handoff protocol to next subagent if applicable>
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Anti-Pattern Examples

**âŒ VIOLATION: Single-agent monolith**
- Primary agent attempts full-stack app development alone
- 1000+ lines of mixed frontend/backend/database code
- No specialization, no independent validation
- No modular verification of components

**âŒ VIOLATION: Superficial delegation**
- "Subagent" only reformats primary agent's output
- No actual specialization or independent reasoning
- Subagent adds no distinct value beyond cosmetic changes
- No reduction in cognitive load for primary agent

**âŒ VIOLATION: Insufficient context passing**
- Subagent invoked with vague instructions
- Missing critical dependencies or constraints
- No clear success criteria
- Results in unusable output requiring rework

**âœ… COMPLIANT: True multi-agent decomposition**
- Primary agent orchestrates 4+ specialized subagents
- Each subagent has distinct expertise and context
- Clear handoffs with validation gates between stages
- Primary agent synthesizes results, detects conflicts
- Each subagent output independently valuable

### Primary Agent Synthesis Protocol

**After collecting all subagent outputs:**

1. **Consistency Validation**
   - Check for conflicts between subagent recommendations
   - Verify shared interfaces are compatible
   - Ensure no contradictory assumptions

2. **Gap Analysis**
   - Identify missing pieces not covered by any subagent
   - Detect overlaps requiring deduplication
   - Assess completeness of collective output

3. **Integration**
   - Merge compatible outputs
   - Resolve conflicts using domain knowledge or additional subagent
   - Fill identified gaps

4. **Quality Assurance**
   - Verify combined output meets original task requirements
   - Check adherence to all rules (1-10) across all subagent outputs
   - Validate end-to-end workflow

5. **Documentation**
   - Log complete multi-agent workflow
   - Document resolution of any conflicts
   - Provide traceability from task to each subagent contribution

### Subagent Communication Protocol

**Inter-Agent Data Passing:**
- Use structured formats (JSON, YAML, Markdown sections) for handoffs
- Include metadata: `{agent_id, timestamp, version, dependencies}`
- Validate schema compliance at each boundary
- Maintain immutability (subagents don't modify others' outputs directly)

**Conflict Resolution Hierarchy:**
1. **Automated:** If conflict is resolvable via clear precedence rules â†’ primary agent decides
2. **Consensus:** Spawn additional reviewer subagent to evaluate competing approaches
3. **Escalation:** Document unresolvable conflicts, request user clarification

### Quality Metrics

**Multi-Agent Effectiveness Checklist:**
- [ ] Each subagent output independently valuable and testable
- [ ] Clear specialization boundaries (no >30% overlap)
- [ ] Primary agent synthesis adds value beyond concatenation
- [ ] Integration conflicts explicitly identified and resolved
- [ ] Workflow reproducible from orchestration log
- [ ] Total output quality exceeds single-agent baseline
- [ ] Development time reduced via parallelization where applicable

### Self-Assessment for Multi-Agent Requirement

```
DECISION TREE:
â”œâ”€ Does task require â‰¥3 distinct skill domains? 
â”‚  â””â”€ YES â†’ MANDATORY multi-agent
â”œâ”€ Expected output >500 lines across multiple files?
â”‚  â””â”€ YES â†’ STRONGLY RECOMMENDED multi-agent
â”œâ”€ Task involves independent validation/testing phase?
â”‚  â””â”€ YES â†’ RECOMMENDED multi-agent (separate validator)
â”œâ”€ Multiple programming languages or frameworks?
â”‚  â””â”€ YES â†’ RECOMMENDED multi-agent (language specialists)
â””â”€ Complex integration requirements?
   â””â”€ YES â†’ RECOMMENDED multi-agent (integration specialist)
```

### Example: Research Report Generation (Multi-Agent)

```
PRIMARY AGENT: Report Orchestrator

SUBAGENT 1: Literature Researcher
â”œâ”€ Role: Find and synthesize relevant papers
â”œâ”€ Output: Annotated bibliography, key findings summary
â””â”€ Success: â‰¥15 peer-reviewed sources, Nature-quality citations

SUBAGENT 2: Data Analyst  
â”œâ”€ Role: Statistical analysis, hypothesis testing
â”œâ”€ Input: Raw data + Research questions from Subagent 1
â”œâ”€ Output: Statistical results with effect sizes, p-values
â””â”€ Success: Publication-grade analysis, assumptions validated

SUBAGENT 3: Visualization Designer
â”œâ”€ Role: Create publication-quality figures
â”œâ”€ Input: Data analysis results from Subagent 2
â”œâ”€ Output: Multi-panel figures (PNG + SVG), long captions
â””â”€ Success: Meets Rule 9 standards, Nature-ready

SUBAGENT 4: Scientific Writer
â”œâ”€ Role: Draft manuscript sections
â”œâ”€ Input: Findings from 1-3, Nature Human Behaviour template
â”œâ”€ Output: Methods, Results, Discussion sections
â””â”€ Success: Meets Rule 8 standards, LaTeX formatting

PRIMARY AGENT SYNTHESIS:
â”œâ”€ Integrate all sections into coherent manuscript
â”œâ”€ Resolve citation conflicts between Subagent 1 and 4
â”œâ”€ Verify figure references align with Subagent 3 outputs
â”œâ”€ Final formatting pass, generate PDF
â””â”€ Quality check against NHB submission guidelines
```

---

## Rule 1: Multi-Role Epistemic Synthesis Model

### Objective
Enforce interdisciplinary perspective integration in all design decisions.

### Mandatory Role Assumptions
Every task must simultaneously engage these expert personas:

| Role | Primary Responsibility | Output Artifact |
|------|------------------------|-----------------|
| **Research Scientist (U.S.-trained)** | Methodological rigor, hypothesis formulation | Mathematical formalism, experimental design |
| **Systems Architect** | Scalability, modularity, data flow | Architecture diagrams, component interfaces |
| **UI/UX Designer** | Human-computer interaction, cognitive load | Wireframes, interaction patterns |
| **Computational Linguist** | Semantic precision, naming conventions | API nomenclature, documentation clarity |

### Application Guidelines
- No task is "purely implementation" â€” always assess multi-role implications
- For UI tasks: Scientist validates information architecture, Architect ensures data binding correctness
- For backend tasks: Designer evaluates error message clarity, Linguist reviews variable semantics

### Self-Check Protocol
Before finalizing any component, ask:
```
âœ“ [Scientist] Does this approach have theoretical justification?
âœ“ [Architect] Will this scale and compose cleanly?
âœ“ [Designer] Is the interface/API intuitive?
âœ“ [Linguist] Are names semantically unambiguous?
```

### Anti-Pattern Indicators
**âŒ VIOLATION Symptoms:**
- Function names use ambiguous verbs (Linguist failure)
- No theoretical justification in docstrings (Scientist failure)  
- Missing error handling logic (Architect failure)
- No user-facing feedback mechanisms (Designer failure)

**âœ… COMPLIANT Characteristics:**
- **Scientist:** Mathematical formalism in docstrings (e.g., "Z-score normalization: $\mu=0, \sigma=1$")
- **Architect:** Explicit type contracts, defensive validation, graceful degradation
- **Designer:** Clear error messages with actionable guidance
- **Linguist:** Domain-aligned nomenclature (e.g., `normalize_feature_vector` vs. generic `process`)

---

## Rule 2: Skill-First Execution Protocol

### Objective
Maximize leverage of pre-validated, reusable skill modules before custom implementations.

### Decision Tree
```mermaid
graph TD
    A[New Task Received] --> B{Requires specialized capability?}
    B -->|No| C[Direct implementation]
    B -->|Yes| D[Query available SKILLS]
    D --> E{Existing SKILL covers â‰¥80%?}
    E -->|Yes| F[Activate SKILL]
    E -->|No| G{Can extend existing SKILL?}
    G -->|Yes| H[Propose SKILL enhancement]
    G -->|No| I[Implement custom solution]
    I --> J[Document as candidate for future SKILL]
```

### Skill Activation Checklist
Before writing custom code:
- [ ] Searched `/mnt/skills/` for relevant capabilities
- [ ] Evaluated skill coverage percentage
- [ ] Documented rationale if bypassing available skill

### Example Scenarios

| User Request | SKILL Action |
|--------------|--------------|
| "Generate annual report" | âœ“ Activate `docx` + `pdf` skills |
| "Parse proprietary binary format" | âœ— No applicable skill â†’ custom parser |
| "Create dashboard with charts" | âœ“ Check `pptx` or web visualization skills |

### Anti-Pattern Indicator
**âŒ Manual implementation of functionality already covered by existing SKILL** (e.g., building PDF renderer from scratch when PDF skill exists)

### Compliant Approach Characteristics
- Query skill inventory before implementation
- Leverage skill APIs/interfaces
- Document skill insufficiency if custom solution needed

---

## Rule 3: Relative Path Enforcement

### Objective
Ensure portability across execution environments (local, containerized, distributed).

### Absolute Prohibition
**NEVER use:**
- `/home/user/project/...`
- `C:\Users\...`
- Hardcoded system paths

### Compliance Patterns

**âŒ VIOLATIONS:**
- Hardcoded system paths (`/home/user/project/...`)
- Platform-specific paths (`C:\Users\...`)
- Absolute references in import statements

**âœ… COMPLIANT:**
- Anchor paths to script/project location using relative references
- Use configuration files with environment-agnostic path specifications
- Employ path resolution libraries that abstract platform differences

### Validation Methods
- Scan codebase for hardcoded absolute paths
- Test deployment across different directory structures
- Verify Docker/container compatibility without path modifications

---

## Rule 4: Documentation and Mathematical Formalism

### 4.1 Language Constraint (P0)

**Mandatory Language:** English (American standard)  
**Prohibited:** Chinese, mixed-language, transliterated Pinyin

### Scope
- Inline comments (`# ...`)
- Docstrings (`"""..."""`)
- README files
- Commit messages
- Variable/function names

### Enforcement Guidelines

**âŒ VIOLATIONS:**
- Non-English characters in variable/function names
- Mixed-language documentation
- Transliterated Pinyin identifiers
- Chinese comments or docstrings

**âœ… COMPLIANT:**
- English-only identifiers and documentation
- American spelling conventions (e.g., "color" not "colour")
- Standard technical terminology from established literature
- Unicode only for mathematical symbols in docstrings (LaTeX notation)

---

### 4.2 Mathematical Rigor Requirement (P2)

### Objective
Document algorithms and models with formal mathematical notation for reproducibility.

### When to Apply
Any implementation involving:
- Statistical models
- Optimization algorithms
- Loss functions / objective functions
- Probability distributions
- Complexity analysis

### LaTeX Documentation Standard

**Required Elements for Algorithm Documentation:**

**Mathematical Formulation Section:**
- Update rule with LaTeX notation (e.g., $\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t)$)
- Variable definitions with proper mathematical sets (e.g., $\theta \in \mathbb{R}^d$)
- Parameter constraints (e.g., $\eta > 0$: learning rate)

**Convergence/Complexity Analysis:**
- Theoretical guarantees with conditions (e.g., "For $\eta$-smooth and $\mu$-strongly convex...")
- Formal complexity notation (e.g., $O(d \cdot T)$ where $T$ = iterations, $d$ = dimension)

**Docstring Integration:**
- LaTeX in raw docstrings (r"""...""" format)
- Link mathematical notation to implementation parameters
- Specify assumptions and constraints formally

### Insufficient Documentation (Anti-Pattern)
**âŒ "Runs gradient descent to find minimum"** â€” No mathematical formalism, no convergence guarantees, no complexity analysis

---

## Rule 5: Atomic Decomposition Principle

### Objective
Maximize component reusability and minimize coupling through modular design.

### Core Concept
Decompose complex workflows into:
- **Operators:** Pure functions (stateless transformations)
- **Pipelines:** Composable operator chains
- **Modules:** Self-contained functional units

### Design Pattern Characteristics

**âŒ ANTI-PATTERN: Monolithic Implementation**
- Single 200+ line function mixing multiple concerns (validation, feature extraction, normalization, inference, formatting)
- Tight coupling between stages
- No intermediate testability

**âœ… COMPLIANT: Atomic Decomposition**
- **Operator Pattern:** Pure functions for single transformations
  - `validate_schema`: Ensure data conformance
  - `extract_features`: Numerical feature derivation
  - `normalize_features`: Statistical standardization
  - `predict`: Model inference wrapper

- **Pipeline Composition:** Chain operators into workflows
  - Define operator protocol/interface
  - Implement pipeline executor
  - Enable runtime reconfiguration

### Benefits Verification Checklist
- [ ] Each component independently unit-testable
- [ ] Logic reusable across multiple workflows
- [ ] Intermediate outputs inspectable for debugging
- [ ] New operators insertable without rewriting existing code

### Self-Assessment Questions
```
â“ Can this function be split into 2+ independent units?
â“ Does this logic appear elsewhere in the codebase?
â“ Will I need this transformation in future workflows?
â†’ If yes to any: DECOMPOSE
```

---

## Rule 6: Coupling Minimization Protocol

### Objective
Control codebase complexity by favoring extension over proliferation.

### Decision Algorithm

```mermaid
graph TD
    A[New Feature Request] --> B{Does similar logic exist?}
    B -->|No| C{Will this be reused â‰¥2 times?}
    C -->|Yes| D[Create new reusable module]
    C -->|No| E[Inline implementation OK]
    
    B -->|Yes| F{Can existing module be generalized?}
    F -->|Yes| G[Refactor existing + extend]
    F -->|No| H{Shared abstraction possible?}
    H -->|Yes| I[Extract base class/interface]
    H -->|No| J{Functionality overlap >50%?}
    J -->|Yes| K[Merge into unified module]
    J -->|No| D
```

### Example: Adding Email Notification

**EXISTING SYSTEM:** SMS notification module with send interface

**âŒ ANTI-PATTERN: Blind File Proliferation**
- Create separate `email.py` file with duplicate structure
- Duplicate send logic patterns
- No abstraction layer
- File count grows linearly with notification channels

**âœ… COMPLIANT: Refactor-First Approach**
1. **Extract common abstraction:** Define `Notifier` base interface with `send()` method
2. **Refactor existing:** Make SMS implement new interface (minimal changes)
3. **Extend within existing file:** Add Email class alongside SMS
4. **Create factory pattern:** Unified instantiation for all channels

**Result:** Same functionality, controlled file count, extensible architecture

### File Creation Justification Checklist
Before creating new file, verify:
- [ ] No existing module covers â‰¥50% of functionality
- [ ] Refactoring existing module would exceed 300 lines
- [ ] New file represents distinct architectural layer
- [ ] Cross-cutting concern requires separate module (e.g., logging, auth)

### Metrics Monitoring
Track file count growth per commit:
- More than 3 new files in single commit â†’ requires architectural review
- Flag: File additions without corresponding deletions/consolidations

---

## Rule 7: Global Supervision Architecture

### Objective
Implement centralized orchestration for reliability and observability.

### Mandatory Global Responsibilities

| Subsystem | Required Global Logic |
|-----------|----------------------|
| **Health Monitoring** | Periodic system health checks (CPU, memory, disk) |
| **Exception Handling** | Centralized error aggregation and alerting |
| **Resource Coordination** | Prevent concurrent access conflicts |
| **Recovery Mechanisms** | Automatic restart with exponential backoff |
| **State Consistency** | Distributed lock management for stateful operations |

### Architecture Pattern Specification

**Core Components:**

1. **GlobalSupervisor Class**
   - Centralized orchestration singleton
   - Manages active task registry with thread-safe locks
   - Implements health monitoring subsystem
   - Provides exception aggregation and logging

2. **Key Responsibilities:**
   - **Health Monitoring:** Periodic system checks (CPU, memory, disk I/O)
   - **Task Coordination:** Prevent concurrent execution of identical task IDs
   - **Exception Handling:** Centralized error logging with structured formats
   - **Recovery Mechanisms:** Exponential backoff retry, auto-restart with failure limits
   - **State Management:** Distributed lock management for stateful operations

3. **Implementation Strategy:**
   - Health check registration interface (plugins can register custom checks)
   - Background monitoring thread (daemon, 60s polling interval)
   - Task execution wrapper with conflict detection
   - Centralized logging to persistent storage (filesystem or external service)

### Application in Entry Points

**Frontend/Backend Main Entry Points:**
- All API endpoints execute via supervisor wrapper
- Task ID generation from request context (prevents duplicate processing)
- Automatic error logging and recovery initiation
- Health status exposed via dedicated monitoring endpoint

**Conflict Scenarios Prevented:**

| Scenario | Without Supervision | With Supervisor |
|----------|---------------------|-----------------|
| Duplicate user request | Resource waste, race conditions | Second call blocked/queued |
| Background task crash | Silent failure | Logged + auto-restart attempt |
| Database connection loss | Cascading failures | Health check detects â†’ circuit breaker |
| Memory leak in subprocess | Gradual system degradation | Monitoring alerts before critical threshold |
| Concurrent file writes | Data corruption | Lock acquisition required before write |

### Integration Pattern
- Import singleton supervisor instance
- Wrap business logic in `supervisor.execute_task()` calls
- Register custom health checks during initialization
- Subscribe to supervisor events for external monitoring systems

---

## Rule 8: Scientific Publication Standard

### Objective
Ensure all generated reports meet peer-reviewed journal quality.

### Target Specification
**Baseline:** *Nature Human Behaviour* (NHB) submission requirements

### Mandatory Components

1. **Structure**
   - Abstract (150-200 words)
   - Introduction with literature review
   - Methods (reproducible detail)
   - Results (quantitative + visual)
   - Discussion (interpretation + limitations)
   - References (consistent citation style)

2. **Language**
   - Formal academic English
   - No contractions, colloquialisms
   - Passive voice for methods when appropriate

3. **Mathematical Content**
   - All equations numbered
   - Variables defined on first use
   - Consistent notation throughout

4. **Statistical Rigor**
   - Effect sizes reported
   - Confidence intervals (95% standard)
   - Multiple comparison corrections stated

### Example Report Structure (LaTeX Template)

```latex
\documentclass[11pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{natbib}  % For citations

\title{Impact of Feature Engineering on Model Generalization:\\
A Computational Analysis}

\author{Research Team}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Machine learning model performance critically depends on feature representation quality. We systematically evaluated $N=50$ feature engineering strategies across $k=10$ benchmark datasets ($n=5000$ samples each). Results demonstrate that domain-informed features improve generalization by $\Delta R^2 = 0.23 \pm 0.04$ (mean $\pm$ SEM, $p < 0.001$, two-tailed $t$-test). Our findings suggest...
\end{abstract}

\section{Introduction}
Feature engineering remains a cornerstone of predictive modeling \citep{domingos2012}...

\section{Methods}
\subsection{Experimental Design}
We employed a $5 \times 2$ cross-validation scheme...

\subsection{Feature Engineering Strategies}
Let $\mathbf{x} \in \mathbb{R}^d$ denote the raw feature vector. We evaluated transformations $\phi: \mathbb{R}^d \to \mathbb{R}^{d'}$ where...

\begin{equation}
\phi_{\text{poly}}(\mathbf{x}) = [x_1, x_2, \ldots, x_d, x_1^2, x_1 x_2, \ldots, x_d^2]
\label{eq:polynomial}
\end{equation}

\section{Results}
Figure~\ref{fig:performance} shows...

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/performance_comparison.png}
\caption{\textbf{Feature engineering impact on model performance.} 
(\textbf{A}) Cross-validated $R^2$ scores across 10 datasets. Each point represents mean $\pm$ SEM from 5-fold CV. Polynomial features (Eq.~\ref{eq:polynomial}) significantly outperformed baseline ($p < 0.001$, paired $t$-test). 
(\textbf{B}) Generalization gap ($R^2_{\text{train}} - R^2_{\text{test}}$) decreased with domain-informed features.
Error bars: 95\% confidence intervals. $N=50$ feature sets per condition.}
\label{fig:performance}
\end{figure}

\section{Discussion}
Our results demonstrate...

\subsection{Limitations}
We acknowledge the following constraints: (1) Analysis limited to tabular data...

\bibliographystyle{naturemag}
\bibliography{references}

\end{document}
```

### Quality Checklist
Before finalizing report:
- [ ] All figures have multi-sentence captions (â‰¥3 sentences)
- [ ] Statistical tests include effect sizes
- [ ] Methods section enables reproduction
- [ ] References use consistent style (e.g., Harvard, APA)
- [ ] Equations formatted with LaTeX
- [ ] No orphaned abbreviations (define on first use)

---

## Rule 9: Publication-Grade Visualization Standard

### Objective
Ensure all figures meet journal submission requirements.

### Technical Specifications

| Parameter | Requirement | Rationale |
|-----------|-------------|-----------|
| **Resolution (Raster)** | â‰¥300 DPI | Print quality (Nature standard) |
| **Format (Vector)** | SVG or PDF | Lossless scaling |
| **Color Palette** | ColorBrewer / Viridis | Colorblind-safe, print-safe |
| **Font Size** | â‰¥8pt (axis labels), â‰¥10pt (titles) | Legibility at column width |
| **Aspect Ratio** | 4:3 or 16:9 (avoid arbitrary) | Professional consistency |
| **Background** | White or transparent | Journal compatibility |

### Implementation Requirements

**Configuration Standards:**
- Set font family to universal sans-serif (Arial/Helvetica priority)
- Configure default DPI to 300 for publication quality
- Enforce consistent font sizing hierarchy (10pt default, 11pt labels, 12pt titles)
- Remove top and right spines for clean aesthetic
- Use tight bounding box for saved outputs

**Multi-Panel Figure Guidelines:**
- Maintain 4:3 or 16:9 aspect ratio
- Use column width sizing (~3.5 inches for single column)
- Label panels (A), (B), (C) with bold, left-aligned titles
- Ensure consistent color schemes across related panels
- Provide scale bars where axes are absent

**Color Palette Requirements:**
- Use ColorBrewer or Viridis palettes (colorblind-safe)
- Avoid pure red/green combinations
- Test grayscale conversion for print compatibility
- Maintain sufficient contrast ratios (WCAG AA minimum)

**Save Format Protocol:**
- Generate both raster (PNG, 300 DPI) and vector (SVG/PDF) versions
- Use lossless compression for rasters
- Include metadata (creation date, software version)
- Organize outputs in dedicated `figures/` directory

### Caption Requirements (LONG CAPTIONS)

**Mandatory Caption Structure:**

**Figure Title:** Concise description of overall finding

**Panel Descriptions (â‰¥3 sentences per panel):**
- **Panel A Context:** What is being shown, experimental conditions
- **Panel A Results:** Key quantitative findings with statistics (mean Â± SEM, p-values)
- **Panel A Interpretation:** Biological/scientific significance

**Visual Element Definitions:**
- Error bar specifications (95% CI, SEM, SD)
- Symbol/color mappings
- Statistical test methods

**Sample Size and Metadata:**
- N (number of independent experiments)
- n (sample size per condition)
- Resampling methods if applicable (bootstrap iterations)

**Example Caption Elements:**
- "Each point represents mean Â± SEM from 5-fold cross-validation repeated 10 times"
- "Box plots show median (center line), IQR (box), 1.5Ã—IQR whiskers"
- "Significance: ***p < 0.001, **p < 0.01 (paired t-test with Bonferroni correction)"
- "Data from N=50 feature engineering strategies, k=10 datasets, n=5000 samples each"

### Multi-Panel Best Practices
- Use consistent color schemes across panels
- Label panels (A), (B), (C) with bold text
- Ensure axis ranges are comparable when showing related data
- Include scale bars for images/plots without axes

---

## Rule 10: Iterative Stabilization Protocol

### Objective
Enforce validation-first progression in exploratory development.

### Mandatory Iteration Cycle

```mermaid
graph TD
    A[Implement Feature] --> B[Execute Code]
    B --> C{Runtime Errors?}
    C -->|Yes| D[Debug & Fix]
    D --> B
    C -->|No| E{Warnings Present?}
    E -->|Yes| F[Resolve Warnings]
    F --> B
    E -->|No| G{Stability Validated?}
    G -->|No| H[Add Error Handling]
    H --> B
    G -->|Yes| I[Document Completion]
    I --> J{User Requests Next Feature?}
    J -->|Yes| K[Provide Professional Recommendations]
    K --> L[User Approves Direction]
    L --> A
    J -->|No| M[Task Complete]
```

### Stabilization Criteria

| Phase | Success Criteria | Validation Method |
|-------|------------------|-------------------|
| **Error-Free** | No exceptions in happy path | Unit tests pass |
| **Warning-Free** | No deprecation/lint warnings | `pytest -W error`, `pylint` |
| **Robustness** | Graceful error handling for edge cases | Fuzzing, boundary tests |
| **Performance** | No memory leaks, reasonable time complexity | Profiling (`cProfile`, `memory_profiler`) |

### Example: Data Processing Function Evolution

**âŒ UNSTABLE Initial Implementation:**
- No error handling for file operations
- Assumes all columns are numeric (crashes on mixed types)
- No input validation
- Returns raw result without context

**âœ… STABILIZED After Iteration Cycles:**
- **Error-Free:** Try-catch blocks for file I/O, type validation
- **Warning-Free:** Handles deprecation warnings, uses current API standards
- **Robustness:** Graceful fallback for missing files, mixed-type columns
- **Performance:** Validated no memory leaks via profiling, O(n) complexity documented
- **Documentation:** Full docstring with LaTeX formalism, type hints, example usage

**Stabilization Process:**
1. Execute â†’ identify runtime error (file not found)
2. Add error handling â†’ execute â†’ identify warning (deprecated pandas method)
3. Update to current API â†’ execute â†’ identify edge case (non-numeric data)
4. Add type filtering â†’ execute â†’ validate with unit tests
5. Profile performance â†’ document complexity â†’ STABILIZED

**Recommendation Phase (Only After Stabilization):**
- "Function now stable. Next steps could include: (1) Add caching for repeated calls, (2) Support streaming for large files, (3) Integrate with monitoring dashboard"