# Stage 1: Builder
FROM python:3.11-slim as builder

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

WORKDIR /app

# Install build dependencies
RUN (sed -i 's|http://deb.debian.org|https://deb.debian.org|g' /etc/apt/sources.list || true) && \
    (sed -i 's|http://deb.debian.org|https://deb.debian.org|g' /etc/apt/sources.list.d/debian.sources || true) && \
    (sed -i 's|http://security.debian.org|https://security.debian.org|g' /etc/apt/sources.list || true) && \
    (sed -i 's|http://security.debian.org|https://security.debian.org|g' /etc/apt/sources.list.d/debian.sources || true) && \
    apt-get update -o Acquire::Retries=3 && apt-get install -y --no-install-recommends \
    gcc \
    build-essential \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY backend/requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Stage 2: Runtime
FROM python:3.11-slim as runtime

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV PATH="/root/.local/bin:$PATH"

WORKDIR /app

# Copy installed dependencies from builder
COPY --from=builder /root/.local /root/.local

# Install Playwright browsers (if needed for crawler)
RUN (sed -i 's|http://deb.debian.org|https://deb.debian.org|g' /etc/apt/sources.list || true) && \
    (sed -i 's|http://deb.debian.org|https://deb.debian.org|g' /etc/apt/sources.list.d/debian.sources || true) && \
    (sed -i 's|http://security.debian.org|https://security.debian.org|g' /etc/apt/sources.list || true) && \
    (sed -i 's|http://security.debian.org|https://security.debian.org|g' /etc/apt/sources.list.d/debian.sources || true) && \
    apt-get update -o Acquire::Retries=3 && \
    playwright install --with-deps chromium && \
    rm -rf /var/lib/apt/lists/*

# Copy source code
COPY backend ./backend
COPY data ./data

# Create directory for persistence
RUN mkdir -p /app/crawlers_jobdir

# Set the working directory to where scrapy.cfg is (or where we run the command from)
WORKDIR /app/backend/crawlers

# Command to run the crawler
# We use a shell script to handle resumption logic (to be created)
COPY backend/scripts/start_crawler.sh /app/start_crawler.sh
RUN chmod +x /app/start_crawler.sh

CMD ["/app/start_crawler.sh"]
