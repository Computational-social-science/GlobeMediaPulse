# Stage 1: Builder
FROM python:3.11-slim as builder

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

WORKDIR /app

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    build-essential \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY backend/requirements.txt .
RUN pip install --no-cache-dir --user -r requirements.txt

# Stage 2: Runtime
FROM python:3.11-slim as runtime

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV PATH="/root/.local/bin:$PATH"

WORKDIR /app

# Copy installed dependencies from builder
COPY --from=builder /root/.local /root/.local

# Install Playwright browsers (if needed for crawler)
RUN apt-get update && \
    playwright install --with-deps chromium && \
    rm -rf /var/lib/apt/lists/*

# Copy source code
COPY backend ./backend
COPY data ./data

# Create directory for persistence
RUN mkdir -p /app/crawlers_jobdir

# Set the working directory to where scrapy.cfg is (or where we run the command from)
WORKDIR /app/backend/crawlers

# Command to run the crawler
# We use a shell script to handle resumption logic (to be created)
COPY backend/scripts/start_crawler.sh /app/start_crawler.sh
RUN chmod +x /app/start_crawler.sh

CMD ["/app/start_crawler.sh"]
